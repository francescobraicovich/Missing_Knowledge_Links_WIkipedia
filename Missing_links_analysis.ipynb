{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function from file in another directory\n",
    "import sys\n",
    "sys.path.append('functions/')\n",
    "\n",
    "from build_graph import build_graph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph not found. Building a new graph.\n",
      "Graph built. Completing the graph with missing links between existing nodes.\n",
      "First round: Processing nodes to add missing links between existing nodes.\n",
      "Graph completed with new links between already existing nodes.\n",
      "Number of nodes: 137 , Number of edges: 7552 , Number of categories: 165\n"
     ]
    }
   ],
   "source": [
    "depth = 1\n",
    "start_page = \"DBSCAN\" \n",
    "\n",
    "graph, links_dict, categories_dict = build_graph(start_page, depth, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neighbors import get_common_neighbors, get_total_neighbors, get_jaccard_coefficient\n",
    "\n",
    "adjacency_matrix = nx.adjacency_matrix(graph).todense()\n",
    "adjacency_matrix.astype(int)\n",
    "common_neighbors_matrix = get_common_neighbors(adjacency_matrix)\n",
    "total_neighbors_matrix = get_total_neighbors(adjacency_matrix, common_neighbors_matrix)\n",
    "jaccard_similarity_matrix = get_jaccard_coefficient(common_neighbors_matrix, total_neighbors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.01, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.041, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.071, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.102, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.133, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.163, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.194, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.225, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.256, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.286, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.317, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.348, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.378, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.409, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.44, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.47, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.501, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.532, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.562, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.593, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.624, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.654, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.685, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.716, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.747, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.777, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.808, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.839, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.869, N Clusters: 3, Silhouette Score: 0.474\n",
      "Epsilon: 0.9, N Clusters: 3, Silhouette Score: 0.474\n",
      "--------------------------------------------------\n",
      "Final Silhouette Score: 0.4742806452462688\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from dbscan import dbscan_from_similarity\n",
    "\n",
    "# combine the adjacency matrix and the jaccard similarity matrix\n",
    "clustering_matrix = jaccard_similarity_matrix + adjacency_matrix\n",
    "\n",
    "# find the maximum value in the matrix\n",
    "max_val = np.max(clustering_matrix)\n",
    "\n",
    "# set the diagonal to the maximum value\n",
    "np.fill_diagonal(clustering_matrix, max_val)\n",
    "\n",
    "cluster_labels = dbscan_from_similarity(clustering_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n",
      "Cluster sizes: [26, 90, 21, 0]\n",
      "Max cluster size: 90, Min cluster size: 0\n"
     ]
    }
   ],
   "source": [
    "# get the number of clusters and the number of nodes in each cluster\n",
    "n_clusters = len(set(cluster_labels))\n",
    "cluster_sizes = [np.sum(cluster_labels == i) for i in range(-1, n_clusters)]\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f'Cluster sizes: {cluster_sizes}')\n",
    "\n",
    "max_size = max(cluster_sizes)\n",
    "min_size = min(cluster_sizes)\n",
    "\n",
    "print(f\"Max cluster size: {max_size}, Min cluster size: {min_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 nodes in cluster 0, with the start page:\n",
      "DBSCAN\n",
      "Active learning (machine learning)\n",
      "Anomaly detection\n",
      "Artificial neural network\n",
      "Association rule learning\n",
      "Autoencoder\n",
      "Automated machine learning\n",
      "BIRCH\n",
      "Batch learning\n",
      "Bias–variance tradeoff\n",
      "Boosting (machine learning)\n",
      "Bootstrap aggregating\n",
      "CURE algorithm\n",
      "Canonical correlation\n",
      "Cluster analysis\n",
      "Computational learning theory\n",
      "Conditional random field\n",
      "Conference on Neural Information Processing Systems\n",
      "Convolutional neural network\n",
      "Curriculum learning\n",
      "Data clustering\n",
      "Data mining\n",
      "Decision tree learning\n",
      "DeepDream\n",
      "Diffusion model\n",
      "Empirical risk minimization\n",
      "Ensemble learning\n",
      "Feature engineering\n",
      "Feature learning\n",
      "Feedforward neural network\n",
      "Fuzzy clustering\n",
      "Gated recurrent unit\n",
      "Generative adversarial network\n",
      "Grammar induction\n",
      "Graphical model\n",
      "Hierarchical clustering\n",
      "Human-in-the-loop\n",
      "Independent component analysis\n",
      "International Conference on Learning Representations\n",
      "International Conference on Machine Learning\n",
      "K-means algorithm\n",
      "K-means clustering\n",
      "Kernel machines\n",
      "Learning curve (machine learning)\n",
      "Learning to rank\n",
      "List of datasets for machine-learning research\n",
      "List of datasets in computer vision and image processing\n",
      "Local outlier factor\n",
      "Long short-term memory\n",
      "Machine learning\n",
      "Mamba (deep learning architecture)\n",
      "Mean shift\n",
      "Meta-learning (computer science)\n",
      "Multi-agent reinforcement learning\n",
      "Multimodal learning\n",
      "Non-negative matrix factorization\n",
      "OPTICS algorithm\n",
      "Occam learning\n",
      "Online machine learning\n",
      "Ontology learning\n",
      "Outline of machine learning\n",
      "Perceptron\n",
      "Principal component analysis\n",
      "Probably approximately correct learning\n",
      "Proper generalized decomposition\n",
      "Q-learning\n",
      "Random forest\n",
      "Random sample consensus\n",
      "Recurrent neural network\n",
      "Regression analysis\n",
      "Reinforcement learning\n",
      "Reinforcement learning from human feedback\n",
      "Relevance vector machine\n",
      "Restricted Boltzmann machine\n",
      "Rule-based machine learning\n",
      "Self-organizing map\n",
      "Self-play (reinforcement learning technique)\n",
      "Self-supervised learning\n",
      "Semi-supervised learning\n",
      "Sparse dictionary learning\n",
      "State–action–reward–state–action\n",
      "Statistical learning theory\n",
      "Structured prediction\n",
      "Supervised learning\n",
      "Support vector machine\n",
      "Temporal difference learning\n",
      "Transformer (machine learning model)\n",
      "U-Net\n",
      "Unsupervised learning\n",
      "Vapnik–Chervonenkis theory\n"
     ]
    }
   ],
   "source": [
    "# find the cluster of the start page\n",
    "start_page_cluster = cluster_labels[0]\n",
    "\n",
    "# find the nodes in the same cluster as the start page\n",
    "start_page_cluster_nodes = [node for node, cluster in zip(graph.nodes, cluster_labels) if cluster == start_page_cluster]\n",
    "\n",
    "# find the dimension of the start page cluster\n",
    "start_page_cluster_dim = len(start_page_cluster_nodes)\n",
    "\n",
    "print(f\"{start_page_cluster_dim} nodes in cluster {start_page_cluster}, with the start page:\")\n",
    "for node in start_page_cluster_nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity thresholds: \n",
      "Weak quantile threshold: 0.984251968503937\n",
      "Strong quantile threshold: 1.0\n"
     ]
    }
   ],
   "source": [
    "# set the diagonal of the adjacency matrix to 0\n",
    "np.fill_diagonal(adjacency_matrix, 0)\n",
    "boolean_adjacency_matrix = adjacency_matrix > 0\n",
    "masked_similarity_matrix = jaccard_similarity_matrix[boolean_adjacency_matrix]\n",
    "\n",
    "# find weak quantile threshold\n",
    "weak_quantile_threshold = 0.75\n",
    "weak_threshold = np.quantile(masked_similarity_matrix, weak_quantile_threshold)\n",
    "\n",
    "# find strong quantile threshold\n",
    "strong_quantile_threshold = 0.95\n",
    "strong_threshold = np.quantile(masked_similarity_matrix, strong_quantile_threshold)\n",
    "\n",
    "# print the thresholds\n",
    "print(f'Similarity thresholds: ')\n",
    "print(f'Weak quantile threshold: {weak_threshold}')\n",
    "print(f'Strong quantile threshold: {strong_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing link candidates: 1\n"
     ]
    }
   ],
   "source": [
    "from missing_links import find_missing_links_multi_thread\n",
    "\n",
    "missing_link_candidates, missing_link_candidates_matrix = find_missing_links_multi_thread(graph, jaccard_similarity_matrix, cluster_labels, weak_threshold, strong_threshold)\n",
    "\n",
    "print(f\"Number of missing link candidates: {len(missing_link_candidates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_dataset import build_dataset_multi_thread\n",
    "\n",
    "train_df, filtered_categories_dict = build_dataset_multi_thread(adjacency_matrix, jaccard_similarity_matrix, missing_link_candidates_matrix, \n",
    "                                                    common_neighbors_matrix, total_neighbors_matrix, \n",
    "                                                    graph, cluster_labels, categories_dict, df_type='train')\n",
    "\n",
    "missing_link_df, filtered_categories_dict = build_dataset_multi_thread(adjacency_matrix, jaccard_similarity_matrix, missing_link_candidates_matrix,\n",
    "                                                            common_neighbors_matrix, total_neighbors_matrix,graph, cluster_labels, categories_dict,\n",
    "                                                            df_type='missing links', filtered_categories_dict=filtered_categories_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into train and test\n",
    "X = train_df.drop(columns=['node_1', 'node_2', 'link'])\n",
    "y = train_df['link']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "\n",
      "Best Score (roc_auc): \u001b[1m1.0\u001b[0m\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "\n",
      "Best Hyperparameters:\n",
      "alpha: 0\n",
      "max_depth: 5\n",
      "n_estimators: 200\n",
      "objective: binary:logistic\n",
      "scale_pos_weight: 0.24354526089884007\n",
      "\n",
      "Optimal Threshold: 0.7444357\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from tune_model import tune\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "\n",
    "# train the model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# calculate scale_pos_weight\n",
    "scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "\n",
    "# set parameters for grid search\n",
    "space = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'scale_pos_weight': [scale_pos_weight],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'alpha': [0, 0.1]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "best_params, best_model = tune(X, y, space, scoring, \n",
    "                            model, modeltype='clf', search_type='grid', n_iter_random=100,\n",
    "                            n_splits=2, n_repeats=1, random_state=1,\n",
    "                            verbose=True, display_plots=0, refit='roc_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 score: 1.0\n",
      "ROC AUC: 1.0\n",
      "Confusion matrix: \n",
      "[[ 59309      0]\n",
      " [     0 240678]]\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Confusion matrix: \")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing link predictions: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [node_1, node_2, link_probability]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the columns 'node1', 'node2' into arrays\n",
    "node1 = missing_link_df['node_1'].values\n",
    "node2 = missing_link_df['node_2'].values\n",
    "\n",
    "# remove the columns 'node1', 'node2' from the dataframe\n",
    "missing_link_to_preidct_df = missing_link_df.drop(columns=['node_1', 'node_2'])\n",
    "\n",
    "# predict link probabilities for the missing link candidates\n",
    "link_probabilities = best_model.predict_proba(missing_link_to_preidct_df)[:, 1]\n",
    "\n",
    "# create a new dataframe with the columns 'node1', 'node2', 'link_probability', 'similarity' and clusters of the nodes\n",
    "missing_link_predictions_df = pd.DataFrame({\n",
    "    'node_1': node1, \n",
    "    'node_2': node2, \n",
    "    'link_probability': link_probabilities, \n",
    "    'similarity': missing_link_df['similarity'].values, \n",
    "    'cluster_node_1': missing_link_df['cluster_node_1'].values, \n",
    "    'cluster_node_2': missing_link_df['cluster_node_2'].values})\n",
    "\n",
    "# sort the dataframe by link_probability\n",
    "missing_link_predictions_df = missing_link_predictions_df.sort_values(by='link_probability', ascending=False)\n",
    "\n",
    "# keep only the values above 0.5 probability\n",
    "missing_link_predictions_df = missing_link_predictions_df[missing_link_predictions_df['link_probability'] > 0.5]\n",
    "\n",
    "# set pd printing limits to display all the rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "columns_to_display = ['node_1', 'node_2', 'link_probability']\n",
    "print(\"Missing link predictions: \")\n",
    "missing_link_predictions_df[columns_to_display].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing links where either node1 or node2 are in the same cluster as the start page:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [node_1, node_2, link_probability]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find missing links where either node1 or node2 are in the same cluster as the start page\n",
    "start_page_cluster = cluster_labels[0]\n",
    "node1_cluster_mask = missing_link_predictions_df['cluster_node_1'].values == start_page_cluster\n",
    "node2_cluster_mask = missing_link_predictions_df['cluster_node_2'].values == start_page_cluster\n",
    "start_page_cluster_mask = node1_cluster_mask | node2_cluster_mask\n",
    "\n",
    "start_page_cluster_missing_links = missing_link_predictions_df[start_page_cluster_mask]\n",
    "\n",
    "print(f'Missing links where either node1 or node2 are in the same cluster as the start page:')\n",
    "start_page_cluster_missing_links[columns_to_display].head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing links that include the start page:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [node_1, node_2, link_probability]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find missing links that include the start page if node1 or node2 is the start page\n",
    "missing_links_start_page_df = missing_link_predictions_df[(missing_link_predictions_df['node_1'] == start_page) | (missing_link_predictions_df['node_2'] == start_page)]\n",
    "\n",
    "print(f'Missing links that include the start page:')\n",
    "missing_links_start_page_df[columns_to_display].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing links between nodes in different clusters:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [node_1, node_2, link_probability]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find missing links between nodes in different clusters\n",
    "different_cluster_mask = missing_link_predictions_df['cluster_node_1'].values != missing_link_predictions_df['cluster_node_2'].values\n",
    "\n",
    "different_cluster_missing_links_df = missing_link_predictions_df[different_cluster_mask]\n",
    "\n",
    "print(f'Missing links between nodes in different clusters:')\n",
    "different_cluster_missing_links_df[columns_to_display].head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
